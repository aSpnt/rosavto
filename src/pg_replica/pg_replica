#!/usr/bin/env python

# -*- coding: utf-8 -*-

"""
***************************************************************************
    pg_replica.py
    ---------------------
    Date                 : January 2014
    Copyright            : (C) 2014 by NextGIS
    Email                : info at nextgis dot org
***************************************************************************
*                                                                         *
*   This program is free software; you can redistribute it and/or modify  *
*   it under the terms of the GNU General Public License as published by  *
*   the Free Software Foundation; either version 2 of the License, or     *
*   (at your option) any later version.                                   *
*                                                                         *
***************************************************************************
"""

__author__ = 'NextGIS'
__date__ = 'January 2014'
__copyright__ = '(C) 2014, NextGIS'

# This will get replaced with a git SHA1 when you do a git archive

__revision__ = '$Format:%H$'


import re
import os
import sys
import glob
import base64
import codecs
import datetime
import argparse
import logging
import ConfigParser

import psycopg2
import psycopg2.extensions

import requests


psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)
psycopg2.extensions.register_type(psycopg2.extensions.UNICODEARRAY)


logger = logging.getLogger('pg_replica')


class DbError(Exception):

    def __init__(self, message, query=None):
        self.message = unicode(message, 'utf-8')
        self.query = (unicode(query, 'utf-8') if query is not None else None)

    def __str__(self):
        return 'MESSAGE: %s\nQUERY: %s' % (self.message, self.query)


class PgDB:

    rep_schema = 'public'
    rep_table = 'rep_log'

    def __init__(self, host=None, port=None, dbname=None, user=None,
                 passwd=None):

        self.logger = logging.getLogger('pg_replica')
        self.re_ident_ok = re.compile(r"^\w+$")
        self.re_is_integer = re.compile('int[0-9]')
        self.re_is_float = re.compile('float[0-9]')

        self.host = host
        self.port = port
        self.dbname = dbname
        self.user = user
        self.passwd = passwd

        if self.dbname == '' or self.dbname is None:
            self.dbname = self.user

        try:
            self.con = psycopg2.connect(self.con_info())
        except psycopg2.OperationalError, e:
            self.logger.debug(e.message)
            raise DbError(e.message)

        self.has_postgis = self.check_postgis()
        self.logger.debug('PostGIS enabled: %s' % self.has_postgis)

    def con_info(self):
        con_str = ''
        if self.host:
            con_str += "host='%s' " % self.host
        if self.port:
            con_str += 'port=%d ' % self.port
        if self.dbname:
            con_str += "dbname='%s' " % self.dbname
        if self.user:
            con_str += "user='%s' " % self.user
        if self.passwd:
            con_str += "password='%s' " % self.passwd
        return con_str

    def check_postgis(self):
        c = self.con.cursor()
        self._exec_sql(c,
            "SELECT COUNT(*) FROM pg_proc WHERE proname = 'postgis_version'")
        return c.fetchone()[0] > 0

    def list_geotables(self, schema=None):
        c = self.con.cursor()

        if schema:
            schema_where = " AND nspname = '%s' " % self._quote_str(schema)
        else:
            schema_where = \
                " AND (nspname != 'information_schema' AND nspname !~ 'pg_') "

        if not self.has_postgis:
            sql = '''SELECT pg_class.relname, pg_namespace.nspname,
                            pg_class.relkind, pg_get_userbyid(relowner),
                            reltuples, relpages, NULL, NULL, NULL, NULL
                  FROM pg_class
                  JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace
                  WHERE pg_class.relkind IN ('v', 'r') ''' \
                  + schema_where + 'ORDER BY nspname, relname'
        else:
            sql = '''SELECT pg_class.relname, pg_namespace.nspname,
                            pg_class.relkind, pg_get_userbyid(relowner),
                            reltuples, relpages, pg_attribute.attname,
                            pg_attribute.atttypid::regtype, NULL, NULL
                  FROM pg_class
                  JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace
                  LEFT OUTER JOIN pg_attribute ON
                      pg_attribute.attrelid = pg_class.oid AND
                      (pg_attribute.atttypid = 'geometry'::regtype
                      OR pg_attribute.atttypid IN
                          (SELECT oid FROM pg_type
                           WHERE typbasetype='geometry'::regtype))
                  WHERE pg_class.relkind IN ('v', 'r') ''' \
                  + schema_where + 'ORDER BY nspname, relname, attname'

        self._exec_sql(c, sql)
        items = c.fetchall()

        if self.has_postgis:
            sql = '''SELECT relname, nspname, relkind,
                            pg_get_userbyid(relowner), reltuples, relpages,
                            geometry_columns.f_geometry_column,
                            geometry_columns.type,
                            geometry_columns.coord_dimension,
                            geometry_columns.srid
                  FROM pg_class
                  JOIN pg_namespace ON relnamespace=pg_namespace.oid
                  LEFT OUTER JOIN geometry_columns ON
                      relname=f_table_name AND nspname=f_table_schema
                  WHERE (relkind = 'r' or relkind='v') ''' \
                  + schema_where + 'ORDER BY nspname, relname, \
                  f_geometry_column'
            self._exec_sql(c, sql)

            for (i, geo_item) in enumerate(c.fetchall()):
                if geo_item[7]:
                    items[i] = geo_item

        return items

    def get_table_fields(self, table, schema=None):
        c = self.con.cursor()
        schema_where = (" AND nspname='%s' "
                        % self._quote_str(schema) if schema is not None else ''
                       )
        sql = '''SELECT a.attnum AS ordinal_position,
                        a.attname AS column_name,
                        t.typname AS data_type,
                        a.attlen AS char_max_len,
                        a.atttypmod AS modifier,
                        a.attnotnull AS notnull,
                        a.atthasdef AS hasdefault,
                        adef.adsrc AS default_value
              FROM pg_class c
              JOIN pg_attribute a ON a.attrelid = c.oid
              JOIN pg_type t ON a.atttypid = t.oid
              JOIN pg_namespace nsp ON c.relnamespace = nsp.oid
              LEFT JOIN pg_attrdef adef ON adef.adrelid = a.attrelid
                  AND adef.adnum = a.attnum
              WHERE
                  c.relname = '%s' %s AND
                  a.attnum > 0
              ORDER BY a.attnum''' % (self._quote_str(table), schema_where)

        self._exec_sql(c, sql)
        attrs = c.fetchall()
        return attrs

    def register_table(self, schema, table):
        if not self.create_replog_table():
            return False

        table_name = self._table_name(schema, table)
        repl_table = self._table_name(self.rep_schema, self.rep_table)

        fields = self.get_table_fields(table, schema)
        has_uuid = len(
            [f for f in fields if f[1] == 'uniq_uid' and \
                                  f[2] == 'varchar']) > 0
        if not has_uuid:
            self.logger.error('Can not register table "%s" because it has '
                              'no "uniq_id" field of type "varchar".'
                              % table_name)
            return False

        proc_name = table + '_rep'

        sql = '''CREATE OR REPLACE FUNCTION %s() RETURNS trigger AS $%s$
                     BEGIN
                         IF (TG_OP = 'INSERT') THEN
                             INSERT INTO %s(
                                 table_name, operation, stamp, uid)
                             VALUES('%s', 2, current_timestamp, NEW.uniq_uid);
                             RETURN NEW;
                         ELSIF (TG_OP = 'UPDATE') THEN
                             INSERT INTO %s(
                                 table_name, operation, stamp, uid)
                             VALUES('%s', 3, current_timestamp, OLD.uniq_uid);
                             RETURN NEW;
                         ELSIF (TG_OP = 'DELETE') THEN
                             INSERT INTO %s(
                                 table_name, operation, stamp, uid)
                             VALUES('%s', 1, current_timestamp, OLD.uniq_uid);
                             RETURN NEW;
                         END IF;
                     END;
                 $%s$ LANGUAGE plpgsql;
                 CREATE TRIGGER %s AFTER INSERT OR UPDATE OR DELETE
                     ON %s FOR EACH ROW EXECUTE PROCEDURE %s();
              ''' % (proc_name, proc_name,
                     repl_table, table_name,
                     repl_table, table_name,
                     repl_table, table_name,
                     proc_name,
                     proc_name,
                     table_name, proc_name)
        self._exec_sql_and_commit(sql)
        return True

    def prepare_changes(self, timestamp, boundary, url_field):
        records = self.list_changes(timestamp)

        data = dict()
        for table, uid in records:
            if table not in data:
                data[table] = [uid]
            else:
                data[table].append(uid)

        repl_table = self._table_name(self.rep_schema, self.rep_table)

        c = self.con.cursor()

        for full_table_name, uids in data.iteritems():
            schema = full_table_name.split('.')[0]
            table = full_table_name.split('.')[1]

            ddl = self.dump_table(schema, table)
            if ddl == '':
                self.logger.critical('Can not get DDL for table "%s".' % table)
                return

            fields = self.get_table_fields(table, schema)
            plain_field_list = []
            select_field_list = []
            for count, field in enumerate(fields):
                field_name = field[1]
                field_type = field[2]
                if field_name == 'ogc_fid':
                    continue
                if field_type in ['geometry']:
                    select_field_list.append(
                        "encode(ST_AsEWKB(%s), 'hex')" % field_name)
                    plain_field_list.append(field_name)
                    geom_name = field_name
                    continue
                plain_field_list.append(field_name)
                select_field_list.append(field_name)

            geom_index = plain_field_list.index(geom_name)

            for uid in uids:
                sql = '''SELECT *
                         FROM %s
                         WHERE uid='%s'
                         ORDER BY stamp DESC
                         LIMIT 1;
                      ''' % (repl_table, uid)
                self._exec_sql(c, sql)
                result = c.fetchone()

                operation = result[1]

                if operation != 1:
                    sql = "SELECT %s FROM %s WHERE uniq_uid='%s';" % (
                        ', '.join(self._quote_field_list(select_field_list)),
                        full_table_name, uid)
                    self._exec_sql(c, sql)
                    values = c.fetchone()

                    value_list = []
                    for (count, value) in enumerate(values):
                        if count == geom_index:
                            value_list.append(
                                "ST_GeomFromEWKB(decode('%s', 'hex'))" % value)
                            continue
                        value_list.append(value)

                    values = self.populate_values(
                        fields, value_list, plain_field_list)

                if operation == 1:
                    self.logger.debug('Found DELETE operation.')

                    sql = "DELETE FROM %s WHERE uniq_uid='%s'" % (
                        full_table_name, uid)
                    self.logger.debug(
                        'Replication SQL: %s' % ' '.join(sql.split()))
                elif operation == 2:
                    self.logger.debug('Found INSERT operation.')

                    sql = 'INSERT INTO %s(%s) VALUES(%s);' % (
                        full_table_name,
                        ', '.join(self._quote_field_list(plain_field_list)),
                        values)
                    self.logger.debug(
                        'Replication SQL: %s' % ' '.join(sql.split()))
                elif operation == 3:
                    self.logger.debug('Found UPDATE operation.')

                    sql = '''UPDATE %s
                           SET (%s) = (%s)
                           WHERE uniq_uid='%s';
                        ''' % (full_table_name,
                               ', '.join(self._quote_field_list(plain_field_list)),
                               values,
                               uid)
                    self.logger.debug(
                        'Replication SQL: %s' % ' '.join(sql.split()))

                encoded_table = base64.urlsafe_b64encode(full_table_name)

                ddl = ' '.join(ddl.split()).encode('utf-8')
                encoded_ddl = base64.urlsafe_b64encode(ddl)

                sql = ' '.join(sql.split()).encode('utf-8')
                encoded_sql = base64.urlsafe_b64encode(sql)

                payload = {
                          'table': encoded_table,
                          'ddl': encoded_ddl,
                          'sql': encoded_sql
                         }

                #sql = '''SELECT %s
                #         FROM %s
                #         WHERE ST_Intersects(wkb_geometry,
                #             (SELECT wkb_geometry
                #              FROM %s
                #              WHERE uniq_uid = '%s'))
                #      ''' % (url_field, boundary, full_table_name, uid)
                #self._exec_sql(c, sql)
                #urls = c.fetchall()

                #if urls is None or len(urls) == 0:
                #    self.logger.error('No intersections with zones found.')
                #    continue

                #for url in urls:
                #    r = requests.post(url, params=payload)

                r = requests.post(
                    'http://192.168.255.106:8000',
                    data=payload)
                self.logger.debug('Server responce: %s' % r.status_code)

        self.clear_old_records(timestamp)

    def apply_changes(self, directory):
        file_list = sorted(glob.glob(os.path.join(directory, '*.changes')))
        self.logger.debug('Found %s changesets.' % len(file_list))

        cfg = ConfigParser.SafeConfigParser()

        for file_name in file_list:
            self.logger.debug('Processing file: "%s".' % file_name)
            cfg.readfp(codecs.open(file_name, encoding='utf-8'))

            full_table_name = base64.urlsafe_b64decode(
                cfg.get('changeset', 'table').encode('utf-8'))
            self.logger.debug('Decoded table name: %s' % full_table_name)
            ddl = base64.urlsafe_b64decode(
                cfg.get('changeset', 'ddl').encode('utf-8'))
            self.logger.debug('Decoded table DDL: %s' % ddl)
            sql = base64.urlsafe_b64decode(
                cfg.get('changeset', 'sql').encode('utf-8')).decode('utf-8')
            self.logger.debug('Decoded table SQL: %s' % sql)

            schema = full_table_name.split('.')[0]
            table = full_table_name.split('.')[1]

            if not self._table_exists(schema, table):
                self.logger.debug('Table "%s" not found. Try to create it.' %
                    table_name)
                self._exec_sql_and_commit(ddl)

            self._exec_sql_and_commit(sql)

            os.remove(file_name)
            self.logger.debug('File "%s" processed and removed' % file_name)

    def dump_table(self, schema, table):
        c = self.con.cursor()

        full_table_name = self._table_name(schema, table)
        table_name = self._quote_str(table)

        ddl = ''

        sql = "SELECT '%s'::regclass::oid" % full_table_name
        self._exec_sql(c, sql)
        table_oid = c.fetchone()[0]

        if table_oid is None:
            self.logger.critical("Can not resolve table name '%s' to OID." %
                                 table_name)
            return ''

        sql = '''SELECT c.relname AS seq,
                        t.relname AS tab,
                        a.attname AS col
                 FROM pg_class c
                   JOIN pg_depend d on d.objid = c.oid AND
                        d.classid='pg_class'::regclass AND
                        d.refclassid='pg_class'::regclass
                   JOIN pg_class t on t.oid = d.refobjid
                   JOIN pg_namespace n on n.oid = t.relnamespace
                   JOIN pg_attribute a on a.attrelid = t.oid AND
                        a.attnum=d.refobjsubid
                   WHERE c.relkind='S' AND
                         d.deptype='a' AND
                         t.oid = %s
              ''' % table_oid

        self._exec_sql(c, sql)
        sequences = c.fetchall()

        alter_stmt = ''
        if sequences is not None and len(sequences) != 0:
            for s in sequences:
                res = self.dump_sequence(s)
                ddl += res[0] + '\n'
                alter_stmt += res[1] + '\n'

        ddl += 'CREATE TABLE %s (' % table_name

        sql = '''SELECT a.attname as atname, a.attnotnull as notnull,
                        a.atthasdef as hasdef,
                        format_type(a.atttypid, a.atttypmod) as typedefn,
                        ad.adsrc as deflt
                 FROM pg_attribute a
                 LEFT OUTER JOIN pg_type t ON a.atttypid = t.oid
                 LEFT OUTER JOIN pg_attrdef ad ON (ad.adrelid=a.attrelid AND
                                                   ad.adnum=a.attnum)
                 WHERE a.attrelid = %s AND
                       a.attnum > 0
                 ORDER BY attnum
              ''' % table_oid
        self._exec_sql(c, sql)
        fields = c.fetchall()
        for field in fields:
            ddl += '"%s" %s' % (field[0], field[3])
            if field[1]:
                ddl += ' NOT NULL'
            if field[2]:
                ddl += ' DEFAULT %s' % field[4]
            ddl = ddl.strip() + ', '

        sql = '''SELECT relname, oid
                 FROM pg_class
                 WHERE oid = (SELECT indexrelid
                              FROM pg_index i
                              WHERE i.indisprimary AND i.indrelid = %s)
              ''' % table_oid
        self._exec_sql(c, sql)
        pkey = c.fetchone()
        if pkey is not None:
            ddl += 'CONSTRAINT %s PRIMARY KEY (' % pkey[0]
            sql = '''SELECT attname
                     FROM pg_attribute
                     WHERE attrelid = %s
                  ''' % pkey[1]
            self._exec_sql(c, sql)
            fields = c.fetchall()
            for field in fields:
                ddl += '%s, ' % field
            ddl = ddl[:-2] + '));\n'

        ddl += alter_stmt

        sql = '''SELECT pg_catalog.pg_get_indexdef(i.indexrelid, 0, true)
                 FROM pg_catalog.pg_class c,
                      pg_catalog.pg_class c2,
                      pg_catalog.pg_index i
                 WHERE c.oid = %s AND
                       c.oid = i.indrelid AND
                       i.indexrelid = c2.oid AND
                       i.indisprimary = FALSE
                 ORDER BY i.indisunique DESC, c2.relname
              ''' % table_oid
        self._exec_sql(c, sql)
        indices = c.fetchall()
        for idx in indices:
            ddl += idx[0] + '\n'

        self.logger.debug('DDL for relation "%s": %s' % (table_name, ddl))
        return ddl

    def dump_sequence(self, sequence):
        seqMax = sys.maxint
        seqMin = -seqMax - 1

        c = self.con.cursor()

        sql = '''SELECT sequence_name,
                        0 AS start_value,
                        increment_by,
                        CASE WHEN increment_by > 0 AND max_value = %s THEN NULL
                             WHEN increment_by < 0 AND max_value = -1 THEN NULL
                             ELSE max_value
                        END AS max_value,
                        CASE WHEN increment_by > 0 AND min_value = 1 THEN NULL
                             WHEN increment_by < 0 AND min_value = %s THEN NULL
                             ELSE min_value
                        END AS min_value,
                        cache_value,
                        is_cycled
                 FROM %s
              ''' % (seqMax, seqMin, sequence[0])

        self._exec_sql(c, sql)
        seq = c.fetchone()

        maxValue = 'NO MAXVALUE' if seq[3] is None else 'MAXVALUE %s' % seq[3]
        minValue = 'NO MINVALUE' if seq[3] is None else 'MINVALUE %s' % seq[4]
        cycle = '' if not seq[6] else 'CYCLE'

        ddl = '''CREATE SEQUENCE %s
                 START WITH %s
                 INCREMENT BY %s
                 %s
                 %s
                 CACHE %s
                 %s;\n
              ''' % (seq[0],
                     seq[1],
                     seq[2],
                     minValue,
                     maxValue,
                     seq[5],
                     cycle
                    )

        ddl2 = '''ALTER SEQUENCE %s OWNED BY %s.%s;''' % (sequence[0],
            sequence[1], sequence[2])

        return (' '.join(ddl.split()), ddl2)

    def list_changes(self, timestamp):
        c = self.con.cursor()

        table_name = self._table_name(self.rep_schema, self.rep_table)
        sql = '''SELECT DISTINCT table_name, uid
                 FROM %s
                 WHERE stamp < '%s'::timestamp with time zone
                 ORDER BY table_name;
              ''' % (table_name, timestamp)

        self._exec_sql(c, sql)
        attrs = c.fetchall()
        return attrs

    def clear_old_records(self, timestamp):
        table_name = self._table_name(self.rep_schema, self.rep_table)
        sql = '''DELETE FROM %s
                 WHERE stamp < '%s'::timestamp with time zone;
              ''' % (table_name, timestamp)

        self._exec_sql_and_commit(sql)
        self.logger.info('Old records removed.')
        return True

    def create_replog_table(self):
        schema = self.rep_schema
        table = self.rep_table
        table_name = self._table_name(schema, table)

        if self._table_exists(schema, table):
            self.logger.debug(
                '"%s" table already exists. Skipping creation.' % table_name)
            return True

        sql = '''CREATE TABLE %s (
                     table_name character varying(255) NOT NULL,
                     operation smallint NOT NULL,
                     stamp timestamp with time zone NOT NULL DEFAULT
                         ('now'::text)::timestamp(2) with time zone,
                     uid CHARACTER VARYING(50) NOT NULL)
              WITH (OIDS=FALSE);
              ALTER TABLE %s OWNER TO %s;
              COMMENT ON COLUMN %s.operation IS
                  '1 - DELETE, 2 - INSERT, 3 - UPDATE';
              ''' % (table_name, table_name, self.user, table_name)

        self._exec_sql_and_commit(sql)
        self.logger.info('Created table "%s".' % table_name)
        return True

    def populate_values(self, fields, values, field_list):
        out = u''
        for count, field_name in enumerate(field_list):
            field_defn = [f for f in fields if f[1] == field_name][0]
            field_type = field_defn[2]
            value = values[count]

            if field_type in ['text', 'varchar']:
                if value is not None:
                    out += u"'%s', " % self._quote_str(value)
                else:
                    out += u'NULL, '
            elif field_type == 'bool':
                if value is not None:
                    out += u'%s, ' % value
                else:
                    out += u'NULL, '
            elif field_type == 'uuid':
                if value is not None:
                    out += u"'%s', " % value
                else:
                    out += u'NULL, '
            elif self.re_is_integer.match(field_type) is not None or \
                    self.re_is_float.match(field_type) is not None:
                if value is not None:
                    out += '%s, ' % value
                else:
                    out += 'NULL, '
            elif field_type in ['numeric']:
                if value is not None:
                    out += u'%s, ' % value
                else:
                    out += u'NULL, '
            elif field_type in ['timestamp', 'timestamptz', 'date', 'time',
                                'interval']:
                if value is not None:
                    out += u"'%s', " % value
                else:
                    out += u'NULL, '
            elif field_type in ['geometry']:
                if value is not None:
                    out += u'%s, ' % value
                else:
                    out += u'NULL, '

        out = out[:-2]
        return out

    def _table_exists(self, schema, table):
        table_name = self._table_name(schema, table)

        tables = self.list_geotables(schema)
        if len([t for t in tables if t[0] == table]) > 0:
            self.logger.debug('Found "%s" table.' % table_name)
            return True
        else:
            self.logger.debug('Table "%s" not found.' % table_name)
            return False

    def _exec_sql(self, cursor, sql):
        try:
            self.logger.debug('Execute query: "%s"' % ' '.join(sql.split()))
            cursor.execute(sql)
        except psycopg2.Error, e:
            raise DbError(e.message, e.cursor.query)

    def _exec_sql_and_commit(self, sql):
        try:
            c = self.con.cursor()
            self._exec_sql(c, sql)
            self.con.commit()
        except DbError, e:
            self.logger.debug(e.message)
            self.con.rollback()
            raise

    def _quote(self, identifier):
        identifier = unicode(identifier)

        if self.re_ident_ok.match(identifier) is not None:
            return identifier

        return u'"%s"' % identifier.replace('"', '""')

    def _quote_str(self, txt):
        txt = unicode(txt)
        return txt.replace("'", "''")

    def _quote_field_list(self, field_list):
        quoted_fields = []
        for field in field_list:
            if field.startswith('encode') or field.startswith('ST_GeomFrom'):
                quoted_fields.append(field)
                continue
            quoted_fields.append('"%s"' % field)
        return quoted_fields

    def _table_name(self, schema, table):
        if not schema:
            return self._quote(table)
        else:
            return u'%s.%s' % (self._quote(schema), self._quote(table))


def register_table(host, port, dbname, user, passwd, full_table_name):
    logger.debug('register_table entered.')

    schema = full_table_name.split('.')[0]
    table = full_table_name.split('.')[1]

    db = PgDB(host, port, dbname, user, passwd)
    db.register_table(schema, table)
    logger.info('Table "%s" registered for replication.' % full_table_name)


def create_changesets(host, port, dbname, user, passwd, boundary, url_field):
    logger.debug('create_changesets entered.')
    timestamp = datetime.datetime.now().isoformat()
    logger.debug('Process records for %s.' % timestamp)
    db = PgDB(host, port, dbname, user, passwd)
    db.prepare_changes(timestamp, boundary, url_field)


def apply_changesets(host, port, dbname, user, passwd, directory):
    logger.debug('apply_changesets entered.')
    timestamp = datetime.datetime.now().isoformat()
    logger.debug('Process files for %s.' % timestamp)
    db = PgDB(host, port, dbname, user, passwd)
    db.apply_changes(directory)


if __name__ == '__main__':
    logger.setLevel(logging.DEBUG)

    ch = logging.StreamHandler()
    ch.setLevel(logging.ERROR)

    formatter = logging.Formatter(
        '%(asctime)s %(name)s: %(levelname)s: %(message)s',
        datefmt='%b %d %H:%M:%S')
    ch.setFormatter(formatter)

    logger.addHandler(ch)

    parser = argparse.ArgumentParser(
        description='Replication of the PostgreSQL database.',
        epilog='Report bugs at bugtracker')

    parser.add_argument('-a', '--add', metavar='TABLE', dest='tableName',
        help='Register table TABLE for replication')
    parser.add_argument('-c', '--create', action='store_true',
        dest='createChangesets', help='Create changesets')
    parser.add_argument('-s', '--sync', action='store_true',
        dest='syncChangesets', help='Apply changesets to database')

    if len(sys.argv) <= 1:
        parser.print_usage()

    args = parser.parse_args()

    config_name = '/etc/pg_replica.conf'
    if not os.path.isfile(config_name):
        logger.critical('Configuration file "%s" not found.' % config_name)
        sys.exit(1)

    cfg = ConfigParser.SafeConfigParser(
        {'log_file': '/var/log/pg_replica.log',
         'log_level': 'debug'})

    cfg.read(config_name)
    log_file = cfg.get('logging', 'log_file')
    log_level = cfg.get('logging', 'log_level')
    num_level = getattr(logging, log_level.upper(), None)
    if not isinstance(num_level, int):
        num_level = 10

    fh = logging.FileHandler(log_file, encoding='utf-8')
    fh.setLevel(num_level)
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    logger.info('Start logging.')

    if cfg.has_section('connection'):
        host = cfg.get('connection', 'host')
        port = cfg.getint('connection', 'port')
        dbname = cfg.get('connection', 'database')
        user = cfg.get('connection', 'user')
        passwd = cfg.get('connection', 'password')
    else:
        logger.critical('Connection settings not found in config file.')
        sys.exit(1)

    if cfg.has_section('zones'):
        table_name = cfg.get('zones', 'table_name')
        field_name = cfg.get('zones', 'field_name')
    else:
        logger.critical('Helper table settings not found in config file.')
        sys.exit(1)

    if cfg.has_section('changes'):
        directory = cfg.get('changes', 'dir_name')
    else:
        logger.critical('Changesets directory not found in config file.')
        sys.exit(1)


    if args.tableName:
        register_table(host, port, dbname, user, passwd, args.tableName)
    elif args.createChangesets:
        create_changesets(host, port, dbname, user, passwd,
                          table_name, field_name)
    elif args.syncChangesets:
        apply_changesets(host, port, dbname, user, passwd, directory)

    logger.info('Stop logging.')
    logging.shutdown()
